üê£ Bitte folgen Sie mir f√ºr neue Updates: https://twitter.com/camenduru <br />
üî• Treten Sie bitte unserem Discord-Server bei: https://discord.gg/k5BwmmvJJU <br />
ü•≥ Treten Sie bitte meiner Patreon-Community bei: https://patreon.com/camenduru <br />

## üö¶ In Arbeit üö¶

## ü¶í Colab
| Colab | Info - Modellseite |
| --- | --- |
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/vicuna-13b-GPTQ-4bit-128g.ipynb) | vicuna-13b-GPTQ-4bit-128g <br /> https://vicuna.lmsys.org
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/vicuna-13B-1.1-GPTQ-4bit-128g.ipynb) | vicuna-13B-1.1-GPTQ-4bit-128g <br /> https://vicuna.lmsys.org
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/stable-vicuna-13B-GPTQ-4bit-128g.ipynb) | stable-vicuna-13B-GPTQ-4bit-128g <br /> https://huggingface.co/CarperAI/stable-vicuna-13b-delta
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/gpt4-x-alpaca-13b-native-4bit-128g.ipynb) | gpt4-x-alpaca-13b-native-4bit-128g <br /> https://huggingface.co/chavinlo/gpt4-x-alpaca
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/pyg-7b-GPTQ-4bit-128g.ipynb) | pyg-7b-GPTQ-4bit-128g <br /> https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/koala-13B-GPTQ-4bit-128g.ipynb) | koala-13B-GPTQ-4bit-128g <br /> https://bair.berkeley.edu/blog/2023/04/03/koala
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/oasst-llama13b-GPTQ-4bit-128g.ipynb) | oasst-llama13b-GPTQ-4bit-128g <br /> https://open-assistant.io
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/wizard-lm-uncensored-7b-GPTQ-4bit-128g.ipynb) | wizard-lm-uncensored-7b-GPTQ-4bit-128g <br /> https://github.com/nlpxucan/WizardLM
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/mpt-storywriter-7b-GPTQ-4bit-128g.ipynb) | mpt-storywriter-7b-GPTQ-4bit-128g <br /> https://www.mosaicml.com
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/wizard-lm-uncensored-13b-GPTQ-4bit-128g.ipynb) | wizard-lm-uncensored-13b-GPTQ-4bit-128g <br /> https://github.com/nlpxucan/WizardLM
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/pyg-13b-GPTQ-4bit-128g.ipynb) | pyg-13b-GPTQ-4bit-128g <br /> https://huggingface.co/PygmalionAI/pygmalion-13b
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/falcon-7b-instruct-GPTQ-4bit.ipynb) | falcon-7b-instruct-GPTQ-4bit <br /> https://falconllm.tii.ae/
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/wizard-lm-13b-1.1-GPTQ-4bit-128g.ipynb) | wizard-lm-13b-1.1-GPTQ-4bit-128g <br /> https://github.com/nlpxucan/WizardLM
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-7b-chat-GPTQ-4bit.ipynb) | llama-2-7b-chat-GPTQ-4bit (4bit) <br /> https://ai.meta.com/llama/
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-13b-chat-GPTQ-4bit.ipynb) | llama-2-13b-chat-GPTQ-4bit (4bit) <br /> https://ai.meta.com/llama/ <br /> üö¶ In Arbeit üö¶ bitte probieren Sie llama-2-13b-chat oder llama-2-7b-chat oder llama-2-7b-chat-GPTQ-4bit
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-7b-chat.ipynb) | llama-2-7b-chat (16bit) <br /> https://ai.meta.com/llama/
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-13b-chat.ipynb) | llama-2-13b-chat (8bit) <br /> https://ai.meta.com/llama/
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/redmond-puffin-13b-GPTQ-4bit.ipynb) | redmond-puffin-13b-GPTQ-4bit (4bit) <br /> https://huggingface.co/NousResearch/Redmond-Puffin-13B
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/stable-beluga-7b.ipynb) | stable-beluga-7b (16bit) <br /> https://huggingface.co/stabilityai/StableBeluga-7B
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/doctor-gpt-7b.ipynb) | doctor-gpt-7b (16bit) <br /> https://ai.meta.com/llama/ (https://github.com/llSourcell/DoctorGPT)
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/code-llama-7b.ipynb) | code-llama-7b (16bit) <br /> https://github.com/facebookresearch/codellama
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/code-llama-instruct-7b.ipynb) | code-llama-instruct-7b (16bit) <br /> https://github.com/facebookresearch/codellama
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/code-llama-python-7b.ipynb) | code-llama-python-7b (16bit) <br /> https://github.com/facebookresearch/codellama
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/mistral-7b-Instruct-v0.1-8bit.ipynb) | mistral-7b-Instruct-v0.1-8bit (8bit) <br /> https://mistral.ai/
[![In Colab √∂ffnen](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/mytho-max-l2-13b-GPTQ.ipynb) | mytho-max-l2-13b-GPTQ (4bit) <br /> https://huggingface.co/Gryphe/MythoMax-L2-13b

## ü¶í Colab Pro
Gem√§√ü der Facebook Research LLaMA-Lizenz (nicht-kommerzielle individuelle Lizenz) d√ºrfen wir dieses Modell m√∂glicherweise nicht mit einem Colab Pro-Konto verwenden.
Aber Yann LeCun sagte "GPL v3" (https://twitter.com/ylecun/status/1629189925089296386). Ich bin ein wenig verwirrt. Ist es m√∂glich, dies mit einem nicht kostenlosen Colab Pro-Konto zu verwenden?

## Anleitung
https://www.youtube.com/watch?v=kgA7eKU1XuA

#### ‚ö† Wenn Sie auf einen `IndexError: list index out of range` Fehler sto√üen, setzen Sie bitte die Instruktionsvorlage der Modelle.
![Screenshot 2023-08-28 165206](https://github.com/camenduru/text-generation-webui-colab/assets/54370274/7f619737-eb3e-4368-9b03-65836d1207f0)

## Text Generation Web UI
[https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (Dank an @oobabooga ‚ù§)

## Modell-Lizenzen
| Modell | Lizenz |
| --- | --- |
vicuna-13b-GPTQ-4bit-128g | Von https://vicuna.lmsys.org: Die Online-Demo ist eine Forschungsvorschau, die nur f√ºr nicht-kommerzielle Zwecke bestimmt ist, vorbehaltlich der Modell-[Lizenz](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) von LLaMA, der Nutzungsbedingungen der von OpenAI generierten Daten und der Datenschutzpraktiken von ShareGPT. Bitte kontaktieren Sie uns, wenn Sie m√∂gliche Verst√∂√üe feststellen. Der Code wird unter der Apache License 2.0 ver√∂ffentlicht.
gpt4-x-alpaca-13b-native-4bit-128g | https://huggingface.co/chavinlo/alpaca-native -> https://huggingface.co/chavinlo/alpaca-13b -> https://huggingface.co/chavinlo/gpt4-x-alpaca
llama-2 | https://ai.meta.com/llama/ Llama 2 ist kostenlos f√ºr Forschungs- und kommerzielle Zwecke verf√ºgbar. ü•≥

## Besonderer Dank
Dank an facebookresearch ‚ù§ f√ºr https://github.com/facebookresearch/llama <br />
Dank an lmsys ‚ù§ f√ºr https://huggingface.co/lmsys/vicuna-13b-delta-v0 <br />
Dank an anon8231489123 ‚ù§ f√ºr https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g (GPTQ 4bit Quantisierung von: https://huggingface.co/lmsys/vicuna-13b-delta-v0) <br />
Dank an tatsu-lab ‚ù§ f√ºr https://github.com/tatsu-lab/stanford_alpaca <br />
Dank an chavinlo ‚ù§ f√ºr https://huggingface.co/chavinlo/gpt4-x-alpaca <br />
Dank an qwopqwop200 ‚ù§ f√ºr https://github.com/qwopqwop200/GPTQ-for-LLaMa <br />
Dank an tsumeone ‚ù§ f√ºr https://huggingface.co/tsumeone/gpt4-x-alpaca-13b-native-4bit-128g-cuda (GPTQ 4bit Quantisierung von: https://huggingface.co/chavinlo/gpt4-x-alpaca) <br />
Dank an transformers ‚ù§ f√ºr https://github.com/huggingface/transformers <br />
Dank an gradio-app ‚ù§ f√ºr https://github.com/gradio-app/gradio <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ <br />
Dank an Neko-Institute-of-Science ‚ù§ f√ºr https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b <br />
Dank an gozfarb ‚ù§ f√ºr https://huggingface.co/gozfarb/pygmalion-7b-4bit-128g-cuda (GPTQ 4bit Quantisierung von: https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b) <br />
Dank an young-geng ‚ù§ f√ºr https://huggingface.co/young-geng/koala <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g (GPTQ 4bit Quantisierung von: https://huggingface.co/young-geng/koala) <br />
Dank an dvruette ‚ù§ f√ºr https://huggingface.co/dvruette/oasst-llama-13b-2-epochs <br />
Dank an gozfarb ‚ù§ f√ºr https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g (GPTQ 4bit Quantisierung von: https://huggingface.co/dvruette/oasst-llama-13b-2-epochs) <br />
Dank an ehartford ‚ù§ f√ºr https://huggingface.co/ehartford/WizardLM-7B-Uncensored <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ (GPTQ 4bit Quantisierung von: https://huggingface.co/ehartford/WizardLM-7B-Uncensored) <br />
Dank an mosaicml ‚ù§ f√ºr https://huggingface.co/mosaicml/mpt-7b-storywriter <br />
Dank an OccamRazor ‚ù§ f√ºr https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g (GPTQ 4bit Quantisierung von: https://huggingface.co/mosaicml/mpt-7b-storywriter) <br />
Dank an ehartford ‚ù§ f√ºr https://huggingface.co/ehartford/WizardLM-13B-Uncensored <br />
Dank an ausboss ‚ù§ f√ºr https://huggingface.co/ausboss/WizardLM-13B-Uncensored-4bit-128g (GPTQ 4bit Quantisierung von: https://huggingface.co/ehartford/WizardLM-13B-Uncensored) <br />
Dank an PygmalionAI ‚ù§ f√ºr https://huggingface.co/PygmalionAI/pygmalion-13b <br />
Dank an notstoic ‚ù§ f√ºr https://huggingface.co/notstoic/pygmalion-13b-4bit-128g (GPTQ 4bit Quantisierung von: https://huggingface.co/PygmalionAI/pygmalion-13b) <br />
Dank an WizardLM ‚ù§ f√ºr https://huggingface.co/WizardLM/WizardLM-13B-V1.1 <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ (GPTQ 4bit Quantisierung von: https://huggingface.co/WizardLM/WizardLM-13B-V1.1) <br />
Dank an meta-llama ‚ù§ f√ºr https://huggingface.co/meta-llama/Llama-2-7b-chat-hf <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ (GPTQ 4bit Quantisierung von: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) <br />
Dank an meta-llama ‚ù§ f√ºr https://huggingface.co/meta-llama/Llama-2-13b-chat-hf <br />
Dank an localmodels ‚ù§ f√ºr https://huggingface.co/localmodels/Llama-2-13B-Chat-GPTQ (GPTQ 4bit Quantisierung von: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) <br />
Dank an NousResearch ‚ù§ f√ºr https://huggingface.co/NousResearch/Redmond-Puffin-13B <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/Redmond-Puffin-13B-GPTQ (GPTQ 4bit Quantisierung von: https://huggingface.co/NousResearch/Redmond-Puffin-13B) <br />
Dank an llSourcell ‚ù§ f√ºr https://huggingface.co/llSourcell/medllama2_7b <br />
Dank an MetaAI ‚ù§ f√ºr https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/ <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/CodeLlama-7B-fp16 <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-fp16 <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/CodeLlama-7B-Python-fp16 <br />
Dank an MistralAI ‚ù§ f√ºr https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 <br />
Dank an Gryphe ‚ù§ f√ºr https://huggingface.co/Gryphe/MythoMax-L2-13b <br />
Dank an TheBloke ‚ù§ f√ºr https://huggingface.co/TheBloke/MythoMax-L2-13B-GPTQ (GPTQ 4bit Quantisierung von: https://huggingface.co/Gryphe/MythoMax-L2-13b)<br />

## Medizinischer Haftungsausschluss
HAFTUNGSAUSSCHLUSS: DIESE WEBSITE BIETET KEINE MEDIZINISCHE BERATUNG
Die Informationen, einschlie√ülich, aber nicht beschr√§nkt auf Texte, Grafiken, Bilder und andere Materialien, die auf dieser Website enthalten sind, dienen ausschlie√ülich Informationszwecken. Kein Material auf dieser Website ist als Ersatz f√ºr professionelle medizinische Beratung, Diagnose oder Behandlung gedacht. Suchen Sie immer den Rat Ihres Arztes oder eines anderen qualifizierten Gesundheitsdienstleisters auf, wenn Sie Fragen zu einem medizinischen Zustand oder einer Behandlung haben, und bevor Sie ein neues Gesundheitsprogramm beginnen. Ignorieren Sie niemals professionelle medizinische Ratschl√§ge oder z√∂gern Sie nicht, diese einzuholen, weil Sie etwas auf dieser Website gelesen haben.
